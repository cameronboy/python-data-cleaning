{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import re\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7e9706b48486f8",
   "metadata": {},
   "source": [
    "file_path = \"data/sales_data.xlsx\"\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "dirty_df = excel_data['CA-US']\n",
    "dirty_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "91f427af-3ced-4d85-a787-89292cca7b1b",
   "metadata": {},
   "source": [
    "## We'll define a single function that encapsulates our cleaning steps\n",
    "\n",
    "This is thie signature. The `pd.DataFrame`s aren't strictly necesssary, they just you _what_ the input and outputs will be\n",
    "```{python}\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pass \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ef9bb52-bfe9-4981-9e11-3a42a33a32ad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def remove_non_numbers(value: str, pattern: str=\"[^(\\d|\\.|)]\") -> float:\n",
    "    \"\"\"\n",
    "    Removes non-numeric characters from a given value and returns the cleaned value as a float.\n",
    "\n",
    "    Args:\n",
    "        value (str): The input string to be processed.\n",
    "        pattern (str): A regular expression pattern used to match non-numeric characters. Defaults to \"^[^\\d|\\.)]\" which matches any character that is not a digit, period, or comma.\n",
    "\n",
    "    Returns:\n",
    "        float: The cleaned value after removing non-numeric characters. If the input string contains no numeric characters, returns NaN (Not a Number).\n",
    "        If a number can't be converted\n",
    "\n",
    "    Examples:\n",
    "        >>> remove_non_numbers(\"123\")\n",
    "        123.0\n",
    "        >>> remove_non_numbers(\"123abc\")\n",
    "        123.0\n",
    "        >>> remove_non_numbers(\"1.2,3\")\n",
    "        1.2\n",
    "    \"\"\"\n",
    "    cleaned_value = re.sub(pattern, \"\", str(value))\n",
    "    if cleaned_value == \"\":\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(cleaned_value)\n",
    "    except ValueError as e:\n",
    "        return -11111111.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    General function to clean a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input is a single pandas dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame after processing with general cleaning steps.\n",
    "\n",
    "    Examples:\n",
    "        >>> clean_data(your_df)\n",
    "            # Apply the cleaning function to your DataFrame\n",
    "\n",
    "    Notes:\n",
    "        - This function may not cover all possible cleaning tasks.\n",
    "        - You should adapt this function according to your specific needs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lowercase the column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Trim the DataFrame by removing completely empty rows and columns\n",
    "    df = df.dropna(how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "    # Remove any columns that have a grand total or sub total in the name\n",
    "    column_row_patterns_to_remove = 'total|grand total|subtotal'\n",
    "    df = df[df.columns.drop(list(df.filter(regex=column_row_patterns_to_remove)))]\n",
    "    \n",
    "    # Find out which rows, if any, have a column with the pattern of a total in it\n",
    "    mask = df.apply(lambda row: row.str.lower().str.contains(column_row_patterns_to_remove).any(), axis=1)\n",
    "    # Keep those rows that *don't* have the pattern in it. \n",
    "    df = df[~mask]\n",
    "\n",
    "    # Fix the on_time, it appears that there are a number of patterns indicating yes/no\n",
    "    # We'll accomplish this with a map.\n",
    "    true_false_map = {\n",
    "        \"y\": True,\n",
    "        \"yes\": True,\n",
    "        \"n\": False,\n",
    "        \"no\": False,\n",
    "        \"N\": False,\n",
    "        \"Y\": True,\n",
    "        \"Yes\": True,\n",
    "        \"No\": False,\n",
    "        1: True,\n",
    "        0: False,\n",
    "        \"1\": True,\n",
    "        \"0\": False,\n",
    "        \"true\": True,\n",
    "        \"false\": False\n",
    "    }\n",
    "    df['on_time'] = df['on_time'].map(true_false_map)\n",
    "    \n",
    "    # Let remove the duplciates from order id \n",
    "    df = df.drop_duplicates(subset='order id', keep='first')\n",
    "    \n",
    "    # Every column besides order id, date, and on time should be numbers based\n",
    "    # on our knowledge of this file. There are a few non-number characters but \n",
    "    # in the event that there are no non-number characters, this is a good \n",
    "    # step to do anyway, it won't hurt to be cautious. \n",
    "\n",
    "    non_number_columns = ['order id','date','on_time']\n",
    "    number_columns = df.columns.difference(non_number_columns)\n",
    "    \n",
    "    df[number_columns] = df[number_columns].map(remove_non_numbers)\n",
    "    \n",
    "    # There are some additional values we can get out of order id such as the \n",
    "    # year and region.  us do that.\n",
    "    \n",
    "    df['region'] = df['order id'].str.extract(r'([A-z]{2})')[0]\n",
    "    df['year'] = df['order id'].str.extract(r'((?!-)\\d{4}(?=-))')[0]\n",
    "    df['date'] = df['date'] + \"/\" + df['year']\n",
    "    \n",
    "    # Let's unpivot this dataframe so it's easier to aggregate down the line.\n",
    "    df = df.melt(\n",
    "        id_vars=['order id', 'date', 'on_time','year','region'],\n",
    "        value_name='sales value',\n",
    "        var_name='segment_shipping'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # remove any nans from teh sales value field \n",
    "    df = df.dropna(how='any', subset=['sales value'])\n",
    "    \n",
    "    \n",
    "    # Now lets split out the segment_shipping field into it's two variables\n",
    "    df[['segment', 'shipping_class']] = df['segment_shipping'].str.split('_',expand=True)\n",
    "    df['shipping_class'] = df['shipping_class'].str.replace(\"class\", \"\").str.strip()\n",
    "    df = df.drop('segment_shipping', axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    df = df.sort_values(by=['order id'])\n",
    "    df = df[[\n",
    "        'order id',\n",
    "        'date',\n",
    "        'region',\n",
    "        'on_time',\n",
    "        'segment',\n",
    "        'shipping_class',\n",
    "        'sales value',\n",
    "    ]]\n",
    "    df.columns = [x.replace(' ', '_') for x in df.columns]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_data(dirty_df)\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20d12f8367c9312d",
   "metadata": {},
   "source": [
    "\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "clean_data_list = []\n",
    "for sheet_name, data in excel_data.items():\n",
    "    # print(sheet_name, \" - \", data.shape, \"rows, columns\")\n",
    "    clean_df = clean_data(data)\n",
    "    # Adding the name of the sheet to the pandas dataframe\n",
    "    clean_df['sheet_name'] = sheet_name\n",
    "    clean_data_list.append(clean_df)\n",
    "    \n",
    "combined_df = pd.concat(clean_data_list).reset_index(drop=True)\n",
    "\n",
    "combined_df.sample(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e69c7bc9bc0281e",
   "metadata": {},
   "source": [
    "clean_data_list[1].to_csv('fl-mn.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f54b7ff3684fe23",
   "metadata": {},
   "source": [
    "### Let's add this to a local, file-based database.\n",
    "- This little database will live in the same folder as this notebook and can be connected to via tableau, excel, and other tools"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b297506b0c27938",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('sales.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "DROP TABLE IF EXISTS sales_table;\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS sales_table (\n",
    "    order_id TEXT,\n",
    "    date TEXT,\n",
    "    region TEXT,\n",
    "    on_time TEXT,\n",
    "    segment TEXT,\n",
    "    shipping_class TEXT,\n",
    "    sales_value REAL\n",
    ")\n",
    "''')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d999c4f40bf78682",
   "metadata": {},
   "source": [
    "combined_df.to_sql('sales_table', conn, if_exists='replace', index=False)\n",
    "conn.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6fe5e5e6eb4e9d8",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite3 database\n",
    "conn = sqlite3.connect('sales.db')\n",
    "\n",
    "# Read data from the sales table into a pandas DataFrame\n",
    "df = pd.read_sql_query('SELECT * FROM sales_table', conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7733c8cb5afe623",
   "metadata": {},
   "source": [
    "df.to_parquet(\"sales_table.parquet\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a480468c-dc6f-453d-871a-5a70f4d6d515",
   "metadata": {},
   "source": [
    "df.to_csv(\"sales_table.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bf78424-8508-483b-967f-86262b808850",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
